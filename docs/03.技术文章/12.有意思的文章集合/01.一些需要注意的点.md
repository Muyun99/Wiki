---
title: 一些需要注意的点
date: 2021-07-11 22:15:17
permalink: /pages/1b9e2a/
categories:
  - 技术文章
  - 有意思的文章集合
tags:
  - 
---
1、loss reweight 需要做归一化，使得值不变的同时，让样本的权重变化

2、Model ensemble 的方差作为不确定性，均值作为输出

3、引入分割结果的不确定性来给 loss 进行 reweight



弱监督的前景和背

#### 1、图像中的背景区域较为杂乱，导致背景类样本具有较大的类内散度，进而导致背景类自身难以准确建模，同时影响其他语义类别的建模，这个问题如何解决？

1、类别无关的物体定位

2、Positive Unlabel Learning，PU学习

3、引入主动学习引入较为精确的标注

4、类内散度不影响，前景背景类较为相似导致的问题，更加强大的预训练模型

#### 2、样本 label noise 的问题怎么解决？

1、图像和标注进行质量评估，对于学习的影响也会比较大，课程学习，先用简单样本，再用难样本

2、如何改造我们的全监督网络，让其的容错性更加高

3、DNN 在比较大的噪声标注下还是比较鲁棒的（我觉得应当是对于分割来说的），co-training，

4、co-training，Transformer 和 CNN 不同性质的网络

5、label noise主要两个来源：伪标签和众包标注，通过软标记来解决噪声问题

**耿新老师：做伪标记的时候，用一个标记分布，用图神经网络做软标记？**

**co-training + 标记分布，多模型都给一个标记分布**

#### 3、现在的弱监督学习基本还是沿用MIL的建模方式，目前是否有其他的机器学习模型适用于弱监督学习？

1、自学习，带噪声的半监督学习，Transformer和自监督Transformer可能会有更好的效果（DINO）

2、特征空间的规整，在特征学习和后续的分类器做一个 match，新的联合优化的方式

3、标记不完整（半监督，PU学习），不正确（label noise），不精确（image-lavel annotation，MIL，偏标记学习）

#### 4、弱监督下模型复杂度从理论上是否与效果成正相关？不准确、不全面的标注是否更加难以训练较大规模的网络模型？

1、噪声非常大，大规模网络表现不好，例如 DeepLabv2 性能是比较好的，动态调整网络的复杂度

2、Scaled-ViT , 标签噪声有多大，对表示学习的模型会产生什么特征空间，特征空间会产生什么，理论上给一个定义

3、问题复杂度增加了，机器学习模型的复杂度应当增加。用最匹配的模型来解决

#### 5、弱监督学习在什么条件下可以逼近强监督学习的效果？什么场景下可以充分发挥弱监督学习的优势？

1、弱监督学习的问题在于：前背景的界限过于模糊

2、标注强一些，线标或者框标，只有3个点左右的gap，如果只有image-lavel的标注会带来10个点左右的差距，弱监督可以结合半监督，

3、显著性信息的引入，合理地引入先验

4、学习问题的一致性，理论上来讲，数据量是无穷大的时候，可以逼近强监督学习的效果；减小搜索空间：先验知识等等

#### 6、当前弱监督学习的核心挑战是什么？当数据不再能驱得动学习过程，是否应该回归模型驱动的学习方法？

1、王：复杂场景的弱监督：引入视觉规律，显著性，边缘（**有无纹理**），超像素

2、叶：表示学习的挑战是一样的，不完全标注、部分标注、现有的信息来估计不完全的信息，能够处理数据的Varience，传统方法引入也不失为一种策略：几何模型

3、耿：小样本弱监督学习（医疗影像），边际递减效应，不断增加数据获得的收益会逐步递减，回归模型驱动，比如决策树，SVM，压缩感知等等

#### 7、弱监督学习在学术界和工业界的未来研究趋势是什么？是否需要新的benchmark？如何定义新的benchmark?

1、万：主动学习、无监督学习结合

2、肖：工业界的在固定的标注代价下，怎么提升最大的收益，标注的组合，领域迁移的工作

3、王：互联网和医院，抗疫的工作

4、叶：工业界可以利用弱监督学习去获取预训练模型，学术界应该探索新的范式，与无监督和表示学习之间的关系，视频的标注量

5、耿：工业界关心的成本问题，先验知识，模型和数据的匹配，元学习