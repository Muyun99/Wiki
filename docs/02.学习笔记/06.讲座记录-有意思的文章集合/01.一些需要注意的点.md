---
title: 一些需要注意的点
date: 2021-07-11 22:15:17
permalink: /pages/1b9e2a/
categories:
  - 技术文章
  - 有意思的文章集合
tags:
  - 
---
1、loss reweight 需要做归一化，使得值不变的同时，让样本的权重变化

2、Model ensemble 的方差作为不确定性，均值作为输出

3、引入分割结果的不确定性来给 loss 进行 reweight

4、 既然标签不准确，那么引入 label-free 的特征表示作为特征的一部分

Siamese/twin/dual Networks

但是这样的 Encoder 很快就会发现平凡解或者退化解，这样的解就是 Encoder 对于所有的图片都 output 同样的输出，为了解决这种平凡解，有不同的解决方案

- Contrastive Learning：要求不同图像的 view 会
  - 常用的损失是 InfoNCE 
  - 但是其弊端是需要较多的负样本才行
    - 在 SimCLE 方法里，其用了 4096 的Batch Size
    - 在 MoCo 方法里，用了 Momentum Queue 来储存负样本
- SwAV 方法中，将样本 assign 到不同的 cluster 中，并且保证每个 cluster 的数目是大致均衡的

- BYOL 方法中，引入了额外的 MLP 作为 predictor，并且使用了 momentum encoder
  - Momentum encoder
    - 对于 encoder 的权重使用 EMA 的方式进行更新
    - 所以权重不是根据梯度来更新的
    - 但是需要保存两份权重的副本  



一个简单的孪生网络是否能够work？

SimSiam 不需要负样本

Siamese Network 是一个刻画不变性（invariance）的方法

- Invariance：同一物体的两个 view 应当产生相同的输出
- 卷积是平移不变性的归纳偏置，但是更多的不变性（例如颜色尺度旋转等）却较难设计对应的算子
- 在这种角度看，孪生网络提供了一种数据驱动的 baseline
- 所以在没有归纳偏置，例如 ViT 上也能够work，并且 work 得很好（MoCov3）