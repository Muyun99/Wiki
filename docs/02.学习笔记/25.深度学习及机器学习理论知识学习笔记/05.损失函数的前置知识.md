---
title: 损失函数的前置知识
date: 2021-09-14 15:43:53
permalink: /pages/e646c8/
categories:
  - 学习笔记
  - 深度学习及机器学习理论知识学习笔记
tags:
  - 
---
三个基本思路（其实感觉不是很准确哈）

最小二乘法：用于计算损失值

极大似然估计法：用于参数估计

交叉熵：用于衡量信息量之间的差异



如何定量地去衡量真实参数和所估计参数之间的差异？

- 



为什么要用交叉熵？

- 因为模型分布可能是异构的，一个分布是高斯分布，一个分布是泊松分布。
- 异构的概率分布无法直接比较其差异，同构的概率分布可以比较其参数



#### 信息量（Information）

如何计算信息量：

- 定义：$I(x) = -log(p(x)) = log \frac{1}{p(x)}$

- 直观理解：刻画消除不确定性所需要的信息量，所发生的事情概率越大，所带来的信息量越小，反之其信息量越大
- 用途：用于刻画某个事件的信息量



#### 熵（Entropy）

熵既有热力学的概念，又有信息学的概念

- 热力学概念：代表一个系统中的混乱程度
- 信息学概念：用于衡量整体所带来的信息量的大小，也就是衡量一个系统消除不确定性的难度

如何计算信息熵：

- 定义：$H(p):=E(P_f)$，信息熵定义为对该系统的信息量求期望
- 直观理解：一个系统消除不确定性的难度

- 用途：衡量一个概率模型的不确定程度



#### 相对熵（KL 散度，KL Divergence）

如何计算相对熵？

- 定义：$D_{KL}(P||Q):=H(p,q)-H(p) = \sum_1^N[p(x_i)log\frac{1}{p(x_i)}-p(x_i)log\frac{1}{q(x_i)}]$$
- 直观理解：如果分布 $Q$ 想达到分布 $P$ 的话，还差了多少信息量
- 用途：衡量两个概率分布之间的差异
- 特点
  - 其不对称，分布 $P$ 在前则是指以分布 $P$ 当做基准
  - 其大于等于0，当分布 $P$ 与 $Q$ 相等时等于0，不相等时大于0（吉布斯不等式证明）



#### 交叉熵（Cross Entropy）

如何计算交叉熵？

- 定义：$H(p,q) = \sum_{1}^{N}p(x)log\frac{1}{q(x)}$

- 直观理解：想要让分布 $Q$ 与 $P$ 尽量地接近，就可以让 $H(p，q)$ 尽量的小，所以 $H(p,q)$ 本身即可作为损失函数，称之为交叉熵。交叉熵越小，代表两个概率模型间越相近

- 用途：衡量两个概率分布直接的差异

  #### 



参考资料

- [1] [“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”](https://www.bilibili.com/video/BV15V411W7VB)