---
title: mmsegmentation框架解析（下）
date: 2021-04-13 16:06:24
permalink: /pages/759a29/
categories:
  - 计算机视觉
  - 图像分割
tags:
  - 
---
## mmsegmentation 框架解析

### Tutorial 5：训练技巧

#### 1、使用不同的学习率

在语义分割中，有些方法提出若 head 的学习率比 backbone 的学习率大，能够得到更好的性能以及更快的拟合

可以使用以下的配置来让 head 的 LR 是 backbone 的 10 倍

```python
optimizer=dict(
    paramwise_cfg = dict(
        custom_keys={
            'head': dict(lr_mult=10.)}))
```

#### 2、在线难例挖掘

可以使用以下的配置来使用像素级的在线难例挖掘

使用这种方式，只有置信度在 0.7 以下的像素会被用作训练，并且我们将使用至少 100000 像素用做训练，如果没有定义 `thresh` ，那么会使用 `min_kept` 参数来选择训练的像素。

```python
_base_ = './pspnet_r50-d8_512x1024_40k_cityscapes.py'
model=dict(
    decode_head=dict(
        sampler=dict(type='OHEMPixelSampler', thresh=0.7, min_kept=100000)) )
```

#### 3、类别不均衡损失

可以使用以下的配置来为每个类别分类不同的权重，使用以下的配置可以调整 cityscapes 数据集的权重

`class_weight` 将作为 `CrossEntropyLoss` 的 `weight` 权重来计算损失

```python
_base_ = './pspnet_r50-d8_512x1024_40k_cityscapes.py'
model=dict(
    decode_head=dict(
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0,
            # DeepLab used this class weight for cityscapes
            class_weight=[0.8373, 0.9180, 0.8660, 1.0345, 1.0166, 0.9969, 0.9754,
                        1.0489, 0.8786, 1.0023, 0.9539, 0.9843, 1.1116, 0.9037,
                        1.0865, 1.0955, 1.0865, 1.1529, 1.0507])))
```

### Tutorial 6：自定义 Runtime 设置

#### 1、自定义优化设置

##### 1.1 使用 PyTorch 支持的 optimizer

可以使用 PyTorch 支持的 optimizer，但是文档中提到使用 Adam 优化器会使性能下降很多

```python
optimizer = dict(type='Adam', lr=0.0003, weight_decay=0.0001)
```

##### 1.2 使用定制 optimizer

- 创建新目录 `mmseg/core/optimizer`
- 在定义` mmseg/core/optimizer/my_optimizer.py`  文件中定义`MyOptimizer` 类

```python
from .registry import OPTIMIZERS
from torch.optim import Optimizer

@OPTIMIZERS.register_module()
class MyOptimizer(Optimizer):

    def __init__(self, a, b, c)
```

- 在 `mmseg/core/optimizer/__init__.py` 中 import 该类

```python
from .my_optimizer import MyOptimizer
```

- 可以使用 `custom_imports`  来手动 import 该类

```python

custom_imports = dict(imports=['mmseg.core.optimizer.my_optimizer'], allow_failed_imports=False)
```

- 可以在配置中这样使用自定义的 Optimizer

```python
optimizer = dict(type='MyOptimizer', a=a_value, b=b_value, c=c_value)
```

##### 1.3 定制 optimizer 构造函数

```python
from mmcv.utils import build_from_cfg

from mmcv.runner.optimizer import OPTIMIZER_BUILDERS, OPTIMIZERS
from mmseg.utils import get_root_logger
from .my_optimizer import MyOptimizer


@OPTIMIZER_BUILDERS.register_module()
class MyOptimizerConstructor(object):

    def __init__(self, optimizer_cfg, paramwise_cfg=None):

    def __call__(self, model):

        return my_optimizer
```

##### 1.4 额外设置

- 使用梯度裁剪（gradient clip）

```python
optimizer_config = dict(
    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))
```

- 使用动量（momentum schedule）加速拟合

```python
lr_config = dict(
    policy='cyclic',
    target_ratio=(10, 1e-4),
    cyclic_times=1,
    step_ratio_up=0.4,
)
momentum_config = dict(
    policy='cyclic',
    target_ratio=(0.85 / 0.95, 1),
    cyclic_times=1,
    step_ratio_up=0.4,
)
```

#### 2、自定义训练 schedule

默认是 40k/80k schedule，调用的是 MMCV 的 `PolyLrUpdateeHook` 

也支持其他的 LR Schedule，例如 `CosineAnnealing` 以及 `Poly` Schedule

- Step schedule

```
lr_config = dict(policy='step', step=[9, 10])
```

- ConsineAnnealing schedule

```
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=1000,
    warmup_ratio=1.0 / 10,
    min_lr_ratio=1e-5)
```

#### 3、自定义 workflow

- 以下语句意味着运行一个 epoch 训练，

```python
workflow = [('train', 1)]
```

- 以下语句意味着运行一个epoch 训练，运行一个epoch 测试

```python
[('train', 1), ('val', 1)]
```

#### 4、自定义 hook

- 可以使用 MMCV 中实现的 hooks

```python
custom_hooks = [
    dict(type='MyHook', a=a_value, b=b_value, priority='NORMAL')
]
```

- 修改默认的 runtime hooks，以下的 hooks 是没有在 `custom_hooks`  中注册的
  - log_config
  - checkpoint_config
  - evaluation
  - lr_config
  - optimizer_config
  - momentum_config

- Checkopint config


  用户可以设置max_keep_ckpts只保存少量的检查点，或者通过save_optimizer决定是否保存优化器的状态。更多的参数细节如下

```python
checkpoint_config = dict(interval=1)
```

- Log config

  log_config 支持多个log hook，并可以设置间隔。现在，MMCV 支持 `WandbLoggerHook`，`MlflowLoggerHook` 以及 `TensorboardLoggerHook`。

```python
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook'),
        dict(type='TensorboardLoggerHook')
    ])
```

- Evaluation config

```python
evaluation = dict(interval=1, metric='mIoU')
```



