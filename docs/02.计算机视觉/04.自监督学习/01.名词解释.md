介绍互信息之前，先引入信息论的一些概念

##### Information （信息量）

刻画消除不确定性所需要的信息量，发生的事情概率越大，所带来的信息量越小，反之其信息量越大

$I(x) = -log(p(x)) = log \frac{1}{p(x)}$

例如：

- 事件A：扔骰子的点数大于3点，其概率为 $\frac{1}{2}$ 
- 事件B：扔骰子的点数为6，其概率为$\frac{1}{6}$

- 我们认为事件 B 体现的信息量更大

##### Entropy（熵）

信息熵用于衡量整体所带来的信息量的大小，即利用期望进行评估“事件香农信息 量 x 事件概率的累加"，也是信息熵的概念

$H(U) = E[-logp_i] = \sum_{i=1}^n{p_i*log\frac{1}{p_i}}$ 

例如：

- abbbb 其信息熵 $H(U)=-\frac{1}{5}log\frac{1}{5}-\frac{4}{5}log\frac{4}{5} \approx 0.54$

- abcde 其信息熵 $H(U)=5(-\frac{1}{5}log\frac{1}{5}) \approx 1.61$

- 计算得到的 "abcde" 信息熵要大于 "abbbb"，其整体的信息量也更大



##### Cross Entropy（交叉熵）

两个随机变量的熵

$H(p,q) = \sum_{1}^{N}p(x)log\frac{1}{q(x)}$

熵的连锁规则：

$H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)$



##### KL Divergence（KL散度或相对熵）

用于衡量概率分布间的差异，也就是信息熵的差异

$D_{KL}(p||q) = H(p,q)-H(p) = \sum_1^N[p(x_i)log\frac{1}{p(x_i)}-p(x_i)log\frac{1}{q(x_i)}]$

整理一下可得：

$D_{KL}(p||q) = H(p,q)-H(p) = \sum_{1}^N p(x)log\frac{p(x)}{q(x)}$

KL 散度的最大特点是不对称，即$D_{KL}(p||q) \neq D_{KL}(q||p)$



##### JS Divergence（JS散度）

JS 散度的特点是其对称，即$D_{JS}(p||q) \neq D_{JS}(q||p)$



##### Mutual Information（互信息）

已知两个变量$x,y$，若 $p(x,y) = p(x)p(y)$ ，则两个随机变量 $x,y$ 独立。由贝叶斯公式即可得到：

$p(y|x) = p(x,y)/p(x) = p(x)p(y)/p(x) = p(y)$

独立性的判别公式反映了已知 $x$ 的情况下，$y$ 的分布是否会发生改变（能否为 $y$ 带来新的信息）.然而独立性只能表示两变量是否有关系，而不能描述他们的关系强弱。

所以引入互信息来量化的评价随机变量之间依赖关系的强弱。定义互信息 $I(x,y)$:

$I(X,Y) = H(X)-H(X|Y) = \sum_{x\in{X}}\sum_{y\in{Y}}p(x,y)log\frac{p(x,y)}{p(x)p(y)}$

互信息的性质

- 对称性：$I(x_i,y_j)=I(y_j,x_i)$
- 非负性：$I(x_i,y_j) = 0$
- X与Y独立时：$I(X,Y) = 0$

