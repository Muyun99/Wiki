---
title: kaggle-Classify Leaves 竞赛方案学习
date: 2021-06-28 15:59:05
permalink: /pages/e518f8/
categories:
  - 计算机视觉
  - 视觉竞赛
tags:
  - 
---
## kaggle-Classify Leaves 竞赛方案学习

#### 1、[5th 解决方案](https://www.kaggle.com/c/classify-leaves/discussion/244714#1340465)

```
模型：
	seresnext50和resnet50
数据增强：
	resize 320, HorizontalFlip, VerticalFlip, Rotate, RandomBrightnesContrasr, ShiftScaleRotate, Normalize
其他：
	优化器：AdamW 
	学习率调整器：CosineAnnealingLR
	损失函数：CrossEntropy
	5折交叉验证， 最终结果为五折准确率最高平均， 两个网络各自平均后再做平均。
```

#### 2、[7th 解决方案](https://www.kaggle.com/charlesyyun/7th-resnest-resnext-densenet-0-98840)

```
模型：
	ResNeSt50 + ResNeXt50_32x4d + DenseNet161
数据增强：
	train_transform = transforms.Compose([
    # 随机裁剪图像，所得图像为原始面积的0.08到1之间，高宽比在3/4和4/3之间。
    # 然后，缩放图像以创建224 x 224的新图像
    transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),
    transforms.RandomHorizontalFlip(), 
    # 随机更改亮度，对比度和饱和度
    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),
    transforms.ToTensor(),   
    # 标准化图像的每个通道
    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])
	
	训练时的 CutMix 以及 预测时的 TTA
	CutMix: https://github.com/ildoonet/cutmix
	TTA: https://github.com/qubvel/ttach
其他：
	总共训了 30 个 epoch
	优化器：使用 AdamW，torch.optim.AdamW(model.parameters(),lr=1e-4,weight_decay= 1e-3)
	学习率调整：CosineAnnealingLR: CosineAnnealingLR(optimizer,T_max=10)
	交叉验证：使用 5 折交叉验证
```

#### 3、[9th 解决方案](https://www.kaggle.com/yichaohan/classify-leaves-11-models)

```
模型：
    0, resnet50d, input_size: 224 (selected)
    1, efficientnet_b3, input_size: 224 (selected)
    2, resnext50_32x4d, input_size: 224
    3, inception_resnet_v2, input_size: 299 (selected)
    4, vit_base_patch16_224, input_size: 224 (selected)
    5, tf_efficientnet_b3_ns, input_size: 224
    6, tf_efficientnet_b4_ns, input_size: 380 (selected)
    7, resnest200e, input_size 320 (selected)
    8, mixnet_s, input_size 224
    9, mixnet_xl, input_size 224 (selected)
    10, resnest50d, input_size 224
数据增强：
	transforms.Compose([
        transforms.ToTensor(),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0),
        transforms.RandomResizedCrop([CFG.size, CFG.size]),
        transforms.Normalize(
            mean=[0.5, 0.5, 0.5],
            std=[0.5, 0.5, 0.5],
        )
其他：
	总共训了 50 个epoch
	batch_size = 32
	criterion = CrossEntropyLoss()
	optimizer = MADGRAD 
    learning_rate = 1e-4
    weight_decay = 1e-9
    scheduler = 'ReduceLROnPlateau' 
    factor = 0.2 # ReduceLROnPlateau
    patience = 4 # ReduceLROnPlateau
    eps = 1e-6   # ReduceLROnPlateau
    交叉验证：5 折交叉验证
    模型选择：Stacked mean combinations & Weighted average（值得学习）
    
    probs_3D = np.zeros([train.shape[0],num_class,num_models])
    Weighted average 用来检验模型的重要性以及验证stacked mean method足够优秀
```

#### 4、[11th 解决方案](https://www.kaggle.com/seefun/seefun-baseline-using-torchutils-lb0-987)

```
自建的比赛tricks库：https://github.com/seefun/TorchUtils
Baseline:https://github.com/seefun/TorchUtils/blob/master/examples/kaggle_leaves_classification.ipynb



模型：

数据增强：
	train_transform = albumentations.Compose([
    albumentations.RandomRotate90(p=0.5),
    albumentations.Transpose(p=0.5),
    albumentations.Flip(p=0.5),
    albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.0625, rotate_limit=45, border_mode=1, p=0.5),
    tu.randAugment(),
    albumentations.Normalize(),
    # 这里要使用 ToTensorV2()
    AT.ToTensorV2(),
    ])
其他：
	使用混合精度训练，节省时间，甚至能因为大batch而提升性能
	scaler = torch.cuda.amp.GradScaler() # for AMP training
	scaler.scale(loss).backward()
	scaler.step(optimizer)
    scaler.update()
	使用imagenet-21k上预训练的efficientnetv2模型：https://arxiv.org/abs/2104.00298
	mutli-dropout
	LR = 3e-4
	EPOCH = 36
	DECAY_SCALE = 20.0
	pooling时concat maxpooling与avgpooling
	mixup / Label Smoothing作为正则
	余弦学习率下降
	使用Ranger(RAdam+Lookahead+GC)/ RangerLars(Ranger+Lars)优化
```

总结：

1. 比赛数据集存在大量leak，训练测试集大量图片重复或接近，过拟合反而能获得更好结果；
2. 数据集存在很多噪声样本，过拟合这些错误标注反而能获得更高的LB；即LB本身并不可靠，更高的LB不代表真正更高的泛化性能；

#### 5、[12th 解决方案](https://www.kaggle.com/wanglinchen/12th-private-score-0-98704-reference-to-seefun)

```
在 11th 的解决方案上进行微调

1、加入mixup，设为0.1（太大会使模型性能下降），使用
2、epoch 加大到 72
3、在 randAugment 中加入cutout
4、加入LabelSmoothing: tu.LabelSmoothingCrossEntropy()
- https://github.com/seefun/TorchUtils/blob/master/torch_utils/criterion/CrossEntropy.py
```

#### 6、[13th 解决方案](https://www.kaggle.com/kxlyhit/13th-code-and-summary)

```
模型：
	一个seresnext50，两个resnext50
数据增强：
	Resize 224            
    transforms.RandomHorizontalFlip(p=0.5),   #随机水平翻转
    transforms.RandomVerticalFlip(p=0.5),     #除了水平竖直反转之外其他的处理方法貌似都会降低acc       transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
其他：
	lr_scheduler:ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=0.0000001)
	在训练过程中使用 cutmix
	验证和测试的时候使用TTA: tta.ClassificationTTAWrapper(model_1, tta.aliases.flip_transform(),  merge_mode='mean')
```

#### 7、[19th 解决方案](https://www.kaggle.com/liyihang970828/19th-credit-to-seefun)

```
模型：
	resnest50d
其他：
	三折交叉验证
```



#### 8、参考资料

- [Guide-to-ensembling-methods](https://www.kaggle.com/amrmahmoud123/1-guide-to-ensembling-methods)

- [Guide to Pytorch Learning Rate Scheduling](https://www.kaggle.com/isbhargav/guide-to-pytorch-learning-rate-scheduling#6.-CosineAnnealingLR)

- [Mixup, Cutmix, FMix Visualisations](https://www.kaggle.com/virajbagal/mixup-cutmix-fmix-visualisations)